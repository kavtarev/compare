Что такое мониторинг?
это система контроля за предметами, организациями или чем угодно еще, что нас интересует

чтобы построить такую систему нам нужно решить 3 проблемы

откуда брать:
  сущности, которые мы мониторим
  информацию по сущностям
  правила по которым мы соотносим сущности и информацию

поэтому было принято решение разбить систему на 3 сервиса,
каждый из которых решает одну из этих проблем.

# Откуда брать -  реджистри (кхд, ехд).

Сервис интегрируется с различными внешними системами. На данный момент 1с, контур-фокус
в дальнейшем планируется интеграция с crm каркад, гпба.

какие сущности в нем есть сейчас?
- Договоры лизинга, предметы, лизингополучатели, лизингодателя, контакты, НИЛ

Как это работает сейчас. Из 1с системы к нам присылается информация по текущим сущностям внутри 1с
это либо новые сущности либо обновленная информация по присланным ранее

Как это работает?
По кроне раз в х минут мы делаем запрос в 1с на эндпоинт,
по которому нам отдается новая информация по сущностям.
После получения ответа сущности парсятся и сохраняются в бд.
По сущностям контрагентов для обогащения данных делается запрос в контур-фокус

После этого в 1с отправляются id сохраненных сущностей, чтобы подтвердить получение.
нужно это затем чтобы при следующем запросе эти сущности не пришли к нам в ответе от 1с,
если по ним не было изменений. По сути это ретрай разнесенный по времени.

Реджистри - это просто хранилище данных.

После получения новых/изменных данных они отправляются в сервис мониторинг.
Отправляются не все данные а только минимально необходимы набор. Отправляются через рэббит.


# откуда брать информацию по сущностям - трэкинг

сервис - интеграция с x-keeper и в дальнейшем с похожими. x-keeper это сервис по мониторингу gps маячков установленых на ПЛ. Он показывает где был ПЛ, выходил ли на связь и тд. Данные в нем обновляются раз в сутки. В нем настроена система геозон, для удобства поиска и возможность создавать свои геозоны.

Что делает в нашей системе. Рас в сутки запрашиваем инфу от кипера по дате последнего выхода на связь. Предоставляем мониторингу список доступных геозон. Отвечаем на запросы мониторинга.

По сути штука умеющая бегать в кипер и хранящая ответы - кто,где и когда был или не был.

# правила игры - мониторинг

из реджистри в него прилетает минимально необходимый набор полей и сущностей для работы
это какая-то информация по предметам лизинга, что брали, кто брал, кто давал, какой договор и тд

На данный момент мониторятся только предметы лизинга. Что это значит? Тут 2 вопроса. Кто попадает на мониторинг и что за правила игры.

кто попадает?
- предметы лизинга, удовлетворяющие набору требований. У нах должен быть правильный тип, например мы не хотим мониторить стулья или пароходы (об этом будет позже). Они должны быть в лизинге сейчас и было бы хорошо чтобы они не были удалены

правила игры?
Кто знает/слышал про скоринг - тому все понятно. Это маркеры. В нашей системе они называются риск-индикаторы. Это сущности которые создаются пользователями и представляют собой проверки, которые пользователь хочет производить. Например - не находится ли ПЛ в СКФО. Или, скажем, по договору лизинга предмет должен быть в Омске - там ли он? Выходит ли он вообще на связь.

как это работает

раз в день берутся все предметы лизинга стоящие на мониторинге, и все включенные индикаторы. Для каждого пл по каждому индикатору отправляется запрос в трэкинг. Запрос идет через рэбит.Ответ так же через него. В запросе мы передаем id индикатора чтобы в ответе было понятно как обрабатывать данные. Ответ от трэкинга обрабатывается соответствующим РИ, если были нарушены правила указаные в индикаторе - например ПЛ не был в нужной зоне или не выходит на связь, тогда создается задача на проверку данного пл. Если по одному и томуже предмету сработало несколько иддикаторов то все они привязываются к задаче.

Задача - похожа на задачу лк. Такой-же конечный автомат по статусам. С той лишь разницей что ничейную задачу можно взять в работу самостоятельно.

по всему мониторингу - доступен сводный отчет.

# техническая история.

Редкие но важные сообщения между сервисами идут через рэбит. Простые запросы через http.

рэбит - своя обертка над connection manager, потому, что не устраивала реализация из ЛК по причине fanout и жесткости своей. Нужна просто для инициализации - дальнейшее в руках разраба. В сервисе реджистри используется решение от golevelup - тоже, что на аукционе. Та же обертка только в шляпе.

по просьбе девопсов вынесли названия очередей и эксченжей в волт.Как это будет выглядеть на большом объеме - не понятно, на мелком пойдет.

ни на что из ЛК шаред не завязывались, все что было нужно просто своровали себе, чтобы не морочить голову с обновлением версий и тэгами.

В отличие от лк разнесли интерфейсы и реализацию по отдельным репозиториям,
что позволяет проще выпускать тэги, меньше проблем при общении сервисов и фронта.

Для сводного отчета используем два material view, чтобы не считать агрегаты каждый раз. Они обновляются при получении данных от реджистри. Решение просто и рабочее, на больших объемах могут быть проблемы по производительности. Тогда и решим.

За чуть менее очевидные проблемы передаем привет девопсам. 

проблем 0 
Из-за небольшого процента выделенного cpu получали лок потока и как следствие не успевал отрабатывать heartbeat и кубер убивал приклад

проблема тайп орм один из маппинг в 40 раз локально в 15 на деве

лимит рэбита

лимит постгри

инн от одной огрн от предшественника

первая из проблем - это ограничения по расходу памяти. Времена лк когда 4гб получали и горя не знали прошли. Столкнулись с тем что больших загрузках 128мб может не хватить и кубер сложит контейнер. Всем мы знаем что нода прожорлива и свои 60-90мб на старте получит. Тут кстати еще один вопрос - как померить сколько мы памяти получаем. process.memoryUsage и данные кубера расходятся.
После оптимизации и кода и запросов падения стали реже, но были. Повысили лимит до 256.

вторая проблема менее очевидна. В чем суть: проэкт поднят в кубере, каждый из сервисов поднят в 2х инстансах. Они равнозначны и раунд робином получают данные. Проблема в том, что в каждом из 2х инстансов запущены кроны и они будут дублироваться. По словам девопсов разница во времени между контейнерами может быть в пару мс. Чтобы решить эту проблему используем локи в рэдис с ттл в несколько секунд. Создать отдельный сервис чисто под крону - нельзя, потому, что по требованиям
все должно быть в 2х экземплярах и следовательно получим ровно эту же проблему.

на сладкое показать всякие вины у стульев, разные отчеты по одному пл в х кипере, ожидание в миллион лет при обращении в 1с. То что 1с 1 запрос за раз.



выводы:

1) не доверяем входным данным, даже от людей этажом выше
2) если данных много - задумываемся про лимиты технологий, и алгоритмы
3) держим в уме параллельное программирование (гонки и повторы)
3) разработка в одного без контроля и ревью - порочная практика
4) орм + много данных - путь в ад